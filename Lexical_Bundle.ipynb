{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from collections import defaultdict\n",
    "from extract_content import extract_content\n",
    "import json\n",
    "import math, re\n",
    "import string\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read pdf\n",
    "filename = 'taiwan-11-ebook.pdf'\n",
    "with fitz.open(filename) as doc:\n",
    "    pages = [page.getText('dict') for page in doc.pages()]\n",
    "    pages = []\n",
    "    for page in doc.pages():\n",
    "        cont = page.getText('dict')\n",
    "        pages.append(cont)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [ INFO ] start to extract content. #page = 419 \n",
      " [ INFO ] processed finished \n"
     ]
    }
   ],
   "source": [
    "text_content, image_content = extract_content(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = []\n",
    "for i in range(61,63):\n",
    "    test1.append(text_content[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Make a Dictionary with LonelyPlanet from Page 61 to Page 341 \n",
    "Dict = {}\n",
    "Heading = []\n",
    "for i in range(61,341):\n",
    "    for block in text_content[i]:\n",
    "        lines = block['lines']\n",
    "        for line in lines:\n",
    "            spans = line['spans']\n",
    "            for span in spans:\n",
    "                if span['size'] == 9.0:\n",
    "                    title = span['text']\n",
    "                    # if span['text'] not in Heading:\n",
    "                    if title not in Heading:\n",
    "                        Heading.append(span['text'])\n",
    "                        #print(title)\n",
    "                        Dict[title] =''\n",
    "                elif span['size'] == 7.800000190734863:\n",
    "                    english_only = ''.join(x for x in span['text'] if ord(x) < 256)\n",
    "                    # englisch_only = list(english_only)\n",
    "                    Dict[title] += english_only\n",
    "                    #pass\n",
    "with open('lonelyplanet_data.json', 'w') as outfile:\n",
    "    json.dump(Dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lonelyplanet_data.json', 'r') as f:\n",
    "    content = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import re\n",
    "#import json\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def words(text): return re.findall(\"([a-zA-Zâ€™-]+|[0-9]+)\", text)\n",
    "def ngrams(tokens, n): return [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n) ]\n",
    "\n",
    "LB_Dict = {}\n",
    "count_lb = defaultdict(lambda: 0)\n",
    "content = json.load(open('lonelyplanet_data.json', 'r'))\n",
    "\n",
    "for key, value in content.items(): \n",
    "    for sentence in sent_detector.tokenize(content[key]):\n",
    "        LB_Dict[key] = []\n",
    "        for i in [3,4,5]:\n",
    "            for ngram in ngrams(words(sentence),i):\n",
    "                doc = nlp(ngram)\n",
    "                for token in doc[0:1]:\n",
    "                    if token.pos_ != 'AUX' and token.tag_ in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "                        # print(ngram)\n",
    "                        count_lb[ngram] += 1\n",
    "                    elif token.pos_ == 'NOUN':\n",
    "                        count_lb[ngram] += 1\n",
    "                    elif token.pos_ == 'ADJ':\n",
    "                        count_lb[ngram] += 1\n",
    "        lex_bundles = [ (ngram, count) for ngram, count in count_lb.items() if count>=3 ]\n",
    "        for ngram, count in sorted(lex_bundles, key=lambda x: -x[1]):\n",
    "            # print ('\\t'.join([ngram, str(count)]))\n",
    "            LB_Dict[key].append((ngram, str(count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('LB_lonelyplanet.json', 'w') as outfile:\n",
    "    json.dump(LB_Dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
