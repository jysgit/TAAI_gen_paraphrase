{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives reader a concept of our proposed method to extract phrase embeddings.  \n",
    "If you want to get the whole code release and all training scripts, please check [this GitHub repo](https://github.com/NTHU-NLPLAB/TAAI_gen_paraphrase).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we simple load our preprocessed data.  \n",
    "The data contain single sentence in one line. All sentences have been tokenized and lemmatized, so they can be directly fed into Word2Vec to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    f = [line.strip() for line in open(data_path, 'r')]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be a list of processed sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and end at houli horse farm ( ) , or -PRON- could go further on to the lovely pilu buddhist monastery ( p )',\n",
       " 'the dumpling be a big favourite',\n",
       " 'a chill venue where musicindustry type hang , and everyone seem to know everyone else']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = load_data('data/all_hyphened_sent.txt')\n",
    "ret[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pretrained model  \n",
    "For convenience, we choose not to train the whole model from scratch, but finetune the model from other's pretrained model.  \n",
    "Hence, we download Google's Word2Vec pretrained model first. This model is trained on Google News Dataset, which contains about 100 billion vocabularies.  \n",
    "For moe details, please refer to [Google's website](https://code.google.com/archive/p/word2vec/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /Users/joaw/Library/Caches/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc/gdown-3.12.2-py3-none-any.whl\n",
      "Collecting tqdm\n",
      "  Downloading tqdm-4.50.0-py2.py3-none-any.whl (70 kB)\n",
      "\u001b[K     |████████████████████████████████| 70 kB 785 kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: six in ./testenv/lib/python3.7/site-packages (from gdown) (1.15.0)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting requests[socks]\n",
      "  Using cached requests-2.24.0-py2.py3-none-any.whl (61 kB)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Using cached chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Using cached urllib3-1.25.10-py2.py3-none-any.whl (127 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2020.6.20-py2.py3-none-any.whl (156 kB)\n",
      "Collecting idna<3,>=2.5\n",
      "  Using cached idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6; extra == \"socks\"\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: tqdm, filelock, chardet, urllib3, certifi, idna, PySocks, requests, gdown\n",
      "Successfully installed PySocks-1.7.1 certifi-2020.6.20 chardet-3.0.4 filelock-3.0.12 gdown-3.12.2 idna-2.10 requests-2.24.0 tqdm-4.50.0 urllib3-1.25.10\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/Users/joaw/workspace/TAAI_gen_paraphrase/testenv/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: models: File exists\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
      "To: /Users/joaw/workspace/TAAI_gen_paraphrase/models/GoogleNews-vectors-negative300.bin\n",
      "1.65GB [01:13, 22.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "# File size: 1.5G\n",
    "\n",
    "!mkdir models\n",
    "!gdown -O models/GoogleNews-vectors-negative300.bin.gz --id 0B7XkCwpI5KDYNlNUTTlSS21pQmM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip -d models/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune our own model \n",
    "\n",
    "After the model is downloaded, we use [gensim](https://github.com/RaRe-Technologies/gensim) package to help us finetune the model.  \n",
    "  \n",
    "**gensim** is a useful tools to use and train on NLP and IR tasks. It has many famous models implemented, like Word2Vec, Doc2Vec, FastText, ...etc. Here we use gensim's API to load and finetune Word2Vec model.  \n",
    "For more information of gensim, please refer to:\n",
    " - [Official Word2Vec Tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)\n",
    " - [Word2Vec's API documentation](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-3.8.3-cp37-cp37m-macosx_10_9_x86_64.whl (24.2 MB)\n",
      "Collecting scipy>=0.18.1\n",
      "  Using cached scipy-1.5.2-cp37-cp37m-macosx_10_9_x86_64.whl (28.7 MB)\n",
      "Collecting numpy>=1.11.3\n",
      "  Using cached numpy-1.19.2-cp37-cp37m-macosx_10_9_x86_64.whl (15.3 MB)\n",
      "Requirement already satisfied: six>=1.5.0 in ./testenv/lib/python3.7/site-packages (from gensim) (1.15.0)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.2.1.tar.gz (122 kB)\n",
      "\u001b[K     |████████████████████████████████| 122 kB 930 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in ./testenv/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.15.12-py2.py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in ./testenv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./testenv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./testenv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./testenv/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.10)\n",
      "Collecting jmespath<1.0.0,>=0.7.1\n",
      "  Using cached jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Using cached s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n",
      "Collecting botocore<1.19.0,>=1.18.12\n",
      "  Downloading botocore-1.18.12-py2.py3-none-any.whl (6.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.7 MB 9.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./testenv/lib/python3.7/site-packages (from botocore<1.19.0,>=1.18.12->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "Using legacy setup.py install for smart-open, since package 'wheel' is not installed.\n",
      "Installing collected packages: numpy, scipy, jmespath, botocore, s3transfer, boto3, smart-open, gensim\n",
      "    Running setup.py install for smart-open ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed boto3-1.15.12 botocore-1.18.12 gensim-3.8.3 jmespath-0.10.0 numpy-1.19.2 s3transfer-0.3.3 scipy-1.5.2 smart-open-2.2.1\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/Users/joaw/workspace/TAAI_gen_paraphrase/testenv/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# gensim shoud have been installed in the first cell. If it's not correctly installed, please run this command.\n",
    "\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec             # This is Word2Vec's base model in gensim\n",
    "from gensim.models import KeyedVectors         # This stores all vocabulary information\n",
    "# from gensim.models.callbacks import CallbackAny2Vec   # This makes us able to record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a function to setup Word2Vec object with specified parameters.  \n",
    "Since we are using Google's model, we set embedding dimension (`size`) same as the dimension in pretrained model. \n",
    "`min_count` is set to `1` to correctly load pretrained model's vocobulary list.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(training_data, emb_dim=300):\n",
    "    model = Word2Vec(size = emb_dim,\n",
    "                     min_count = 1)\n",
    "    model.build_vocab(training_data)\n",
    "    example_count = model.corpus_count\n",
    "    return model, example_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load Google's pretrained weight into our model.  \n",
    "Since fine-tuning is not the funcionality officially supported by gensim, there are some work sould be done by ourselves:  \n",
    "\n",
    "1. open pretrained model  \n",
    "2. build all pretrained vocabularies into our voc list  \n",
    "3. copy all model weights from pretrained model to our model  \n",
    "\n",
    "Note that it needs a while due to pretrained model's large size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(model, pretrained_path):\n",
    "    pretrained_model = KeyedVectors.load_word2vec_format(pretrained_path, binary=True)\n",
    "    model.build_vocab([list(pretrained_model.vocab.keys())], update=True)\n",
    "    del pretrained_model   # free memory\n",
    "    model.intersect_word2vec_format(pretrained_path, binary=True, lockf=0.0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start to do training. We set 10 as default #ephcos because the model will have the best performance at this setting.  \n",
    "Note that the training progress needs a while, too. (about ? minutes for ? epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, example_count, epochs):\n",
    "    return model.train(training_data,\n",
    "                       total_examples = example_count,\n",
    "                       epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get phrase embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate phrase embeddings, we proposed two method.  \n",
    "We'll introduce two methods and show in following section.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method A\n",
    "Simply extract words in T9856_phrase_all.txt's embeddings from vector.kv file.    \n",
    "You need to create a folder to save the extracted .npy files, and we use 'embeddings' here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/MethodA_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = []\n",
    "with open('data/T8956_phrase_all.txt', 'r') as f:\n",
    "    for lines in f:\n",
    "        lb.append(lines.replace('\\n', ''))\n",
    "\n",
    "word_vectors = KeyedVectors.load('model/w3_a0.025_300_10i/vector.kv')\n",
    "\n",
    "lb_dash = [lbs.replace(' ', '_') for lbs in lb]\n",
    "\n",
    "for lbs in lb_dash:\n",
    "    if lbs in word_vectors:\n",
    "        path = 'embeddings/'+lbs\n",
    "        np.save(path, word_vectors[lbs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting some phrase embeddings, you can now go to [Compare similarities](#Compare-similarities) section to see how similar the phrases are, or you can continue to go through [our Method B](#Method-B) first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another method, differing from hyphening all phrases and train a new embedding model as Method A, we try to extract embeddings of **every words in a phrase**. Then, we use sentence embedding models to **encode those words into a single phrase embedding**, as the picture shows below.  \n",
    "This is reasonable because phrases are actually combinations of words, and their meanings usually come from words.  \n",
    "![](images/MethodB_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we use [InferSent](https://github.com/facebookresearch/InferSent) with Facebook's pretrained model as our sentence embedding model.  \n",
    "Before we start, we should prepare our environment for InferSent first.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in ./testenv/lib/python3.7/site-packages (3.5)\n",
      "Requirement already satisfied: click in ./testenv/lib/python3.7/site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: joblib in ./testenv/lib/python3.7/site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: regex in ./testenv/lib/python3.7/site-packages (from nltk) (2020.9.27)\n",
      "Requirement already satisfied: tqdm in ./testenv/lib/python3.7/site-packages (from nltk) (4.50.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/Users/joaw/workspace/TAAI_gen_paraphrase/testenv/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torch\n",
      "  Using cached torch-1.6.0-cp37-none-macosx_10_9_x86_64.whl (97.4 MB)\n",
      "Requirement already satisfied: numpy in ./testenv/lib/python3.7/site-packages (from torch) (1.19.2)\n",
      "Collecting future\n",
      "  Using cached future-0.18.2.tar.gz (829 kB)\n",
      "Using legacy setup.py install for future, since package 'wheel' is not installed.\n",
      "Installing collected packages: future, torch\n",
      "    Running setup.py install for future ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed future-0.18.2 torch-1.6.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/Users/joaw/workspace/TAAI_gen_paraphrase/testenv/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# packages shoud have been installed in the first cell. If it's not correctly installed, please run these commands.\n",
    "# If you couldn't install pytorch correctly, please refer to official install instructoin (https://pytorch.org/get-started/locally/)\n",
    "\n",
    "!pip install nltk\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/joaw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If this is your first time using nltk, remember download punkt data first\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use InferSent model, we need to download Facebook's pretrained weight first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: encoders: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  146M  100  146M    0     0  12.1M      0  0:00:12  0:00:12 --:--:-- 14.5M    0  0:00:24  0:00:03  0:00:21 6026k\n"
     ]
    }
   ],
   "source": [
    "# File size: 146M\n",
    "\n",
    "!mkdir encoders\n",
    "!curl -Lo encoders/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far the environment is well prepared, so we can directly extract word embeddings from our finetuned word2vec, and throw then into InferSent to get phrase embeddings now.  \n",
    "There are four steps to achieve this.  \n",
    "1. Set up Word2Vec model\n",
    "2. Get all word embeddings\n",
    "3. Set up Inferset model\n",
    "4. Get phrase embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up Word2Vec\n",
    "\n",
    "Now, in the first step, we have to extract all word embedding from Word2Vec model.  \n",
    "Since all utilities functions have been created above, we easily call them to set up our training pipeline.  \n",
    "1. load our training data\n",
    "2. create gensim's Word2Vec model\n",
    "3. load Google's pretrained model (This needs a while)\n",
    "4. finetuning (This also need a while)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_data2 = load_data('data/all_unhyphened_sent.txt')\n",
    "\n",
    "print('Creating model...')\n",
    "w2v, example_count = create_model(training_data2)\n",
    "w2v = load_pretrained_model(w2v, 'models/GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# train model\n",
    "print('training model...')\n",
    "train_model(w2v, example_count, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extrace word embeddings from our model\\!   \n",
    "To make whole progress easier, we here create a function to extract all embeddings from a phrase string in advance. \n",
    "\n",
    "#### Get list of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(model, phrase):\n",
    "    words = phrase.split(' ')\n",
    "    word_embeddings, unfound_words = [], []\n",
    "    for word in words:\n",
    "        try:\n",
    "            emb = model.wv[word]\n",
    "            word_embeddings.append(emb)\n",
    "        except:\n",
    "            unfound_words.append(word)\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple test to check results from `get_word_embeddings`.  \n",
    "You should see a list of embeddings as return.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methodB_phrase1 = 'look for the'\n",
    "word_embs = get_word_embeddings(methodB_phrase1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup InferSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create an InferSent object with specified parameters, and then we load pretrained model we downloaded above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from infersent import InferSent\n",
    "\n",
    "# defaul config of infersent\n",
    "config = {'bsize': 64, \n",
    "          'word_emb_dim': 300, \n",
    "          'enc_lstm_dim': 2048,\n",
    "          'pool_type': 'max', \n",
    "          'dpout_model': 0.0, \n",
    "          'version': 2}\n",
    "\n",
    "infersent = InferSent(config)\n",
    "infersent.load_state_dict(torch.load('encoders/infersent2.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get phrase embeddings\n",
    "\n",
    "Before we use InferSent model, we have to convert word embeddings into InferSent-compatible batch first. We create a function here to do the job.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_batch(word_embs):\n",
    "    # load beginning-of-sent and end-of-sent embedding\n",
    "    emb_bos = np.load(os.path.join('word_embs', 'bos.npy'))\n",
    "    emb_eos = np.load(os.path.join('word_embs', 'eos.npy'))\n",
    "    \n",
    "    # extract embeddings\n",
    "    lengths = len(word_embs) + 2\n",
    "    embeddings = np.stack((emb_bos, word_embs, emb_pos))\n",
    "    \n",
    "    batch = np.zeros((word_len, 1, 300))\n",
    "    for i in range(len(embeddings)):\n",
    "        batch[i][0][:] = embeddings[i]\n",
    "    \n",
    "    return torch.FloatTensor(batches), np.array(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the word embeddings extracted above to check the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, length = transform_batch(word_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the tensor is prepared, we can extract phrase embeddings from InferSent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    methodB_pharse_emb1 = infersent.forward((batch, length)).data.cpu().numpy()\n",
    "print(methodB_pharse_emb1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Method B - All in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methodB_phrase2 = 'check out the'\n",
    "word_embs = get_word_embeddings(methodB_phrase2)\n",
    "batch, length = transform_batch(word_embs)\n",
    "with torch.no_grad():\n",
    "    methodB_pharse_emb2 = infersent.forward((batch, length)).data.cpu().numpy()\n",
    "print(methodB_pharse_emb2)\n",
    "\n",
    "# save embeddings in numpy-format if you want\n",
    "# out_path = 'phrase'\n",
    "# np.save(out_path, phrase_emb.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have two embeddings, we can compare their similarities with cosine similarities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    ret = np.inner(a, b) / (norm(a) * norm(b))\n",
    "    return 0 if np.isnan(ret) else ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase1, phrase2 = methodA_pharse_emb1, methodA_pharse_emb2\n",
    "phrase1, phrase2 = methodB_pharse_emb1, methodB_pharse_emb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{:.3f}'.format(cosine_similarity(phrase1, phrase2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most similar phrases\n",
    "If you want to find the most similar phrases, we have to extract all phrases' embeddings and store them into a folder first. To keep the tutorial simple, we don't do this here in this notebook, you can refer to python scripts in [our GitHub](https://github.com/NTHU-NLPLAB/TAAI_gen_paraphrase) to do the job for you.  \n",
    "Once all phrases are stored in a folder, we can load them and do the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load all phrase embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_folder = ''\n",
    "for filename in os.listdir(folder):\n",
    "    if filename in ['.', '..']: continue\n",
    "    bundle = os.path.splitext(filename)[0].replace('_', ' ')\n",
    "    emb = np.load(os.path.join(folder, filename), allow_pickle=True)\n",
    "    embeddings[bundle] = emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a utility function to help us find and print the most similar phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(target, bundles, n=5):\n",
    "    similarities = []\n",
    "    target_emb = bundles[target]\n",
    "    for bundle, bundle_emb in bundles.items():\n",
    "        if bundle == target: continue\n",
    "        similarities.append((target, bundle, cosine_similarity(target_emb, bundle_emb)))\n",
    "    similarities.sort(key=lambda emb:-emb[2])\n",
    "    return similarities[:n]\n",
    "\n",
    "def print_similarity(tuples):\n",
    "    head = True\n",
    "    for t in tuples:\n",
    "        if head:\n",
    "            print(f'{t[0]}')\n",
    "            head = False\n",
    "        print(f'  > {t[1]}\\t{t[2]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can set up a interactive searching progress. Enjoy\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    query = input('input: ')\n",
    "    if query in ['quit', 'q']: breakn\n",
    "    print_similarity(most_similar(query, bundles_emb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Empty Test",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
