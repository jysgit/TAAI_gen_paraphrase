{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Related work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "related work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we simple load our preprocessed data. \n",
    "The data contain one sentence in one line. All sentences have been tokenized and lemmatized, so they can be directly fed into Word2Vec to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    f = [line.strip() for line in open(data_path, 'r')]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be a list of processed sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = load_data('data/all_hyphened_sent.txt')\n",
    "ret[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download Google's Word2Vec pretrained model. This model is trained on Google News Dataset, which contains about 100 billion words.  \n",
    "You can find more details of this model on [Google's website](https://code.google.com/archive/p/word2vec/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models\n",
    "!gdown -O models/GoogleNews-vectors-negative300.bin --id 0B7XkCwpI5KDYNlNUTTlSS21pQmM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetune our own model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use [gensim](https://github.com/RaRe-Technologies/gensim) to help us train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.callbacks import CallbackAny2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(training_data, emb_dim=300):\n",
    "    model = Word2Vec(size = emb_dim,\n",
    "                     in_count = 1)\n",
    "    model.build_vocab(training_data)\n",
    "    example_count = model.corpus_count\n",
    "    return model, examplt_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load Google's pretrained weight into our model.  \n",
    "(This may need a while.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(model, pretrained_path):\n",
    "    pretrained_model = KeyedVectors.load_word2vec_format(pretrained_path, binary=True)\n",
    "    model.build_vocab([list(pretrained_model.vocab.keys())], update=True)\n",
    "    del pretrained_model   # free memory\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start to do training. We set 10 as default #ephcos because the model will have the best performance at this setting.  \n",
    "Note that the training progress needs a while, too. (about ? minutes for ? epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, example_count, epochs):\n",
    "    return model.train(training_data,\n",
    "                       total_examples = example_count,\n",
    "                       epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get phrase embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple intro of two method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method A\n",
    "Simply extract words in T8956_phrase_all.txt's embeddings from vector.kv file.    \n",
    "You need to create a folder to save the extracted .npy files, and we use 'embeddings' here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = []\n",
    "with open('data/T8956_phrase_all.txt', 'r') as f:\n",
    "    for lines in f:\n",
    "        lb.append(lines.replace('\\n', ''))\n",
    "\n",
    "word_vectors = KeyedVectors.load('model/w3_a0.025_300_10i/vector.kv')\n",
    "\n",
    "lb_dash = [lbs.replace(' ', '_') for lbs in lb]\n",
    "\n",
    "for lbs in lb_dash:\n",
    "    if lbs in word_vectors:\n",
    "        path = 'embeddings/'+lbs\n",
    "        np.save(path, word_vectors[lbs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/MethodA_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another method, differing from hyphened phrases and train a new embedding model, we try to extract word embeddings from every words in a phrase through general word embedding model. Then, we use sentence embedding models to encode those words into one phrase embedding, as the picture shows below. This is reasonable because phrases are actually some combinations of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/MethodB_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we use InferSent and Facebook's released pretrained model as our sentence embedding model.  \n",
    "So we can directly extract word embeddings from our finetuned word2vec, and then trow embeddins into InferSent.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load training data and word2vec pretreained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data2 = load_data('data/all_unhyphened_sent.txt')\n",
    "\n",
    "print('Creating model...')\n",
    "w2v, example_count = create_model(training_data2)\n",
    "w2v = load_pretrained_model(w2v, 'models/GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "# train model\n",
    "print('training model...')\n",
    "train_model(w2v, example_count, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we extrace word embeddings from our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(model, phrase):\n",
    "    words = phrase.split(' ')\n",
    "    word_embeddings, unfound_words = [], []\n",
    "    for word in words:\n",
    "        try:\n",
    "            emb = model.wv[word]\n",
    "            word_embeddings.append(emb)\n",
    "        except:\n",
    "            unfound_words.append(word)\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_phrase = 'look for the'\n",
    "word_embs = get_word_embeddings(my_phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use InferSent model, we need to download Facebook's pretrained weight first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir encoders\n",
    "!curl -Lo encoders/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we load pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from infersent import InferSent\n",
    "\n",
    "# defaul config of infersent\n",
    "config = {'bsize': 64, \n",
    "          'word_emb_dim': 300, \n",
    "          'enc_lstm_dim': 2048,\n",
    "          'pool_type': 'max', \n",
    "          'dpout_model': 0.0, \n",
    "          'version': 2}\n",
    "\n",
    "infersent = InferSent(config)\n",
    "infersent.load_state_dict(torch.load('encoders/infersent2.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use InferSent model, we need to convert word embeddings into batch that is Infersent compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_batch(word_embs):\n",
    "    # load beginning-of-sent and end-of-sent embedding\n",
    "    emb_bos = np.load(os.path.join('word_embs', 'bos.npy'))\n",
    "    emb_eos = np.load(os.path.join('word_embs', 'eos.npy'))\n",
    "    \n",
    "    # extract embeddings\n",
    "    lengths = len(word_embs) + 2\n",
    "    embeddings = np.stack((emb_bos, word_embs, emb_pos))\n",
    "    \n",
    "    batch = np.zeros((word_len, 1, 300))\n",
    "    for i in range(len(embeddings)):\n",
    "        batch[i][0][:] = embeddings[i]\n",
    "    \n",
    "    return torch.FloatTensor(batches), np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, length = transform_batch(word_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can extract phrase embeddings from InferSent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    phrase_emb = infersent.forward((batch, length)).data.cpu().numpy()\n",
    "print(phrase_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use our model for finding similar phrases, we need to extract all phrases' embeddings first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
