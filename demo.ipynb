{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives reader a concept of our proposed method to extract phrase embeddings.  \n",
    "If you want to get the whole code release and all training scripts, please check [this GitHub repo](https://github.com/NTHU-NLPLAB/TAAI_gen_paraphrase).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to install all packages at once, directly run this cell\n",
    "\n",
    "# !pip install -r requirement.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we simple load our preprocessed data.  \n",
    "The data contain single sentence in one line. All sentences have been tokenized and lemmatized, so they can be directly fed into Word2Vec to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    f = [line.strip() for line in open(data_path, 'r', encoding='utf-8')]\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result will be a list of processed sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and end at houli horse farm ( ) , or -PRON- could go further on to the lovely pilu buddhist monastery ( p )',\n",
       " 'the dumpling be a big favourite',\n",
       " 'a chill venue where musicindustry type hang , and everyone seem to know everyone else']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = load_data('data/all_hyphened_sent.txt')\n",
    "ret[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download pretrained model  \n",
    "For convenience, we choose not to train the whole model from scratch, but finetune the model from other's pretrained model.  \n",
    "Hence, we download Google's Word2Vec pretrained model first. This model is trained on Google News Dataset, which contains about 100 billion vocabularies.  \n",
    "For moe details, please refer to [Google's website](https://code.google.com/archive/p/word2vec/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in ./env/lib/python3.8/site-packages (3.12.2)\n",
      "Requirement already satisfied: filelock in ./env/lib/python3.8/site-packages (from gdown) (3.0.12)\n",
      "Requirement already satisfied: six in ./env/lib/python3.8/site-packages (from gdown) (1.15.0)\n",
      "Requirement already satisfied: requests[socks] in ./env/lib/python3.8/site-packages (from gdown) (2.24.0)\n",
      "Requirement already satisfied: tqdm in ./env/lib/python3.8/site-packages (from gdown) (4.50.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests[socks]->gdown) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests[socks]->gdown) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./env/lib/python3.8/site-packages (from requests[socks]->gdown) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./env/lib/python3.8/site-packages (from requests[socks]->gdown) (3.0.4)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in ./env/lib/python3.8/site-packages (from requests[socks]->gdown) (1.7.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM\n",
      "To: /home/nlplab/joaw/projects/TAAI_gen_paraphrase/models/GoogleNews-vectors-negative300.bin.gz\n",
      "1.65GB [01:18, 21.0MB/s]\n"
     ]
    }
   ],
   "source": [
    "# File size: ~1.5G\n",
    "\n",
    "!mkdir models\n",
    "!gdown -O models/GoogleNews-vectors-negative300.bin.gz --id 0B7XkCwpI5KDYNlNUTTlSS21pQmM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gzip -d models/GoogleNews-vectors-negative300.bin.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune our own model \n",
    "\n",
    "After the model is downloaded, we use [gensim](https://github.com/RaRe-Technologies/gensim) package to help us finetune the model.  \n",
    "  \n",
    "**gensim** is a useful tools to use and train on NLP and IR tasks. It has many famous models implemented, like Word2Vec, Doc2Vec, FastText, ...etc. Here we use gensim's API to load and finetune Word2Vec model.  \n",
    "For more information of gensim, please refer to:\n",
    " - [Official Word2Vec Tutorial](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html)\n",
    " - [Word2Vec's API documentation](https://radimrehurek.com/gensim/models/word2vec.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in ./env/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in ./env/lib/python3.8/site-packages (from gensim) (2.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in ./env/lib/python3.8/site-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in ./env/lib/python3.8/site-packages (from gensim) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.18.1 in ./env/lib/python3.8/site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: requests in ./env/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (2.24.0)\n",
      "Requirement already satisfied: boto3 in ./env/lib/python3.8/site-packages (from smart-open>=1.8.1->gensim) (1.15.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in ./env/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in ./env/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.10)\n",
      "Requirement already satisfied: idna<3,>=2.5 in ./env/lib/python3.8/site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied: botocore<1.19.0,>=1.18.13 in ./env/lib/python3.8/site-packages (from boto3->smart-open>=1.8.1->gensim) (1.18.13)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in ./env/lib/python3.8/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in ./env/lib/python3.8/site-packages (from boto3->smart-open>=1.8.1->gensim) (0.3.3)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./env/lib/python3.8/site-packages (from botocore<1.19.0,>=1.18.13->boto3->smart-open>=1.8.1->gensim) (2.8.1)\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# gensim shoud have been installed in the first cell. If it's not correctly installed, please run this command.\n",
    "\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec             # This is Word2Vec's base model in gensim\n",
    "from gensim.models import KeyedVectors         # This stores all vocabulary information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a function to setup Word2Vec object with specified parameters.  \n",
    "Since we are using Google's model, we set embedding dimension (`size`) same as the dimension in pretrained model. \n",
    "`min_count` is set to `1` to correctly load pretrained model's vocobulary list.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(training_data, emb_dim=300):\n",
    "    model = Word2Vec(size = emb_dim,\n",
    "                     min_count = 1)\n",
    "    model.build_vocab(training_data)\n",
    "    example_count = model.corpus_count\n",
    "    return model, example_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load Google's pretrained weight into our model.  \n",
    "Since fine-tuning is not the funcionality officially supported by gensim, there are some work sould be done by ourselves:  \n",
    "\n",
    "1. open pretrained model  \n",
    "2. build all pretrained vocabularies into our voc list  \n",
    "3. copy all model weights from pretrained model to our model  \n",
    "\n",
    "Note that it needs a while due to pretrained model's large size.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(model, pretrained_path):\n",
    "    pretrained_model = KeyedVectors.load_word2vec_format(pretrained_path, binary=True)\n",
    "    model.build_vocab([list(pretrained_model.vocab.keys())], update=True)\n",
    "    del pretrained_model   # free memory\n",
    "    model.intersect_word2vec_format(pretrained_path, binary=True, lockf=0.0)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can start to do training. We set 10 as default #ephcos because the model will have the best performance at this setting.  \n",
    "Note that the training progress needs a while, too. (about ? minutes for ? epoches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(training_data, model, example_count, epochs):\n",
    "    return model.train(training_data,\n",
    "                       total_examples = example_count,\n",
    "                       epochs = epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get phrase embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate phrase embeddings, we proposed two method.  \n",
    "We'll introduce two methods and show in following section.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method A\n",
    "Simply extract words in T9856_phrase_all.txt's embeddings from vector.kv file.    \n",
    "You need to create a folder to save the extracted .npy files, and we use 'embeddings' here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/MethodA_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = []\n",
    "with open('data/T8956_phrase_all.txt', 'r') as f:\n",
    "    for lines in f:\n",
    "        lb.append(lines.replace('\\n', ''))\n",
    "\n",
    "word_vectors = KeyedVectors.load('model/w3_a0.025_300_10i/vector.kv')\n",
    "\n",
    "lb_dash = [lbs.replace(' ', '_') for lbs in lb]\n",
    "\n",
    "for lbs in lb_dash:\n",
    "    if lbs in word_vectors:\n",
    "        path = 'embeddings/'+lbs\n",
    "        np.save(path, word_vectors[lbs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting some phrase embeddings, you can now go to [Compare similarities](#Compare-similarities) section to see how similar the phrases are, or you can continue to go through [our Method B](#Method-B) first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another method, differing from hyphening all phrases and train a new embedding model as Method A, we try to extract embeddings of **every words in a phrase**. Then, we use sentence embedding models to **encode those words into a single phrase embedding**, as the picture shows below.  \n",
    "This is reasonable because phrases are actually combinations of words, and their meanings usually come from words.  \n",
    "![](images/MethodB_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we use [InferSent](https://github.com/facebookresearch/InferSent) with Facebook's pretrained model as our sentence embedding model.  \n",
    "Before we start, we should prepare our environment for InferSent first.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip\n",
      "Collecting click (from nltk)\n",
      "  Using cached https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl\n",
      "Collecting joblib (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/c9/f58220ac44a1592f79a343caba12f6837f9e0c04c196176a3d66338e1ea8/joblib-0.17.0-py3-none-any.whl (301kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 1.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2d/f7/7853ca43f65c6dfb7706b11c960718b90527a2419686b5a2686da904fc3e/regex-2020.9.27-cp38-cp38-manylinux2010_x86_64.whl (675kB)\n",
      "\u001b[K     |████████████████████████████████| 675kB 3.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in ./env/lib/python3.8/site-packages (from nltk) (4.50.0)\n",
      "Installing collected packages: click, joblib, regex, nltk\n",
      "  Running setup.py install for nltk ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed click-7.1.2 joblib-0.17.0 nltk-3.5 regex-2020.9.27\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Collecting torch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/5d/faf0d8ac260c7f1eda7d063001c137da5223be1c137658384d2d45dcd0d5/torch-1.6.0-cp38-cp38-manylinux1_x86_64.whl (748.8MB)\n",
      "\u001b[K     |████████████████████████████████| 748.8MB 59kB/s  eta 0:00:012   |▍                               | 9.9MB 11.4MB/s eta 0:01:06     |██▋                             | 61.4MB 10.4MB/s eta 0:01:07     |██▋                             | 62.0MB 10.4MB/s eta 0:01:07     |███▉                            | 90.3MB 5.9MB/s eta 0:01:53     |████▏                           | 98.5MB 4.9MB/s eta 0:02:13     |██████                          | 142.8MB 4.1MB/s eta 0:02:27     |█████████████                   | 306.0MB 3.5MB/s eta 0:02:07     |████████████████████▍           | 477.6MB 13.2MB/s eta 0:00:21     |█████████████████████▏          | 496.4MB 13.5MB/s eta 0:00:19     |██████████████████████▉         | 535.3MB 7.8MB/s eta 0:00:28     |███████████████████████▉        | 556.5MB 8.4MB/s eta 0:00:23     |███████████████████████████▋    | 646.2MB 10.2MB/s eta 0:00:11\n",
      "\u001b[?25hCollecting future (from torch)\n",
      "  Using cached https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz\n",
      "Requirement already satisfied: numpy in ./env/lib/python3.8/site-packages (from torch) (1.19.2)\n",
      "Installing collected packages: future, torch\n",
      "  Running setup.py install for future ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed future-0.18.2 torch-1.6.0\n",
      "\u001b[33mWARNING: You are using pip version 19.2.3, however version 20.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# packages shoud have been installed in the first cell. If it's not correctly installed, please run these commands.\n",
    "# If you couldn't install pytorch correctly, please refer to official install instructoin (https://pytorch.org/get-started/locally/)\n",
    "\n",
    "!pip install nltk\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/nlplab/joaw/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If this is your first time using nltk, remember download punkt data first\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use InferSent model, we need to download Facebook's pretrained weight first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  146M  100  146M    0     0  11.3M      0  0:00:12  0:00:12 --:--:-- 13.5M\n"
     ]
    }
   ],
   "source": [
    "# File size: 146M\n",
    "\n",
    "!mkdir encoders\n",
    "!curl -Lo encoders/infersent2.pkl https://dl.fbaipublicfiles.com/infersent/infersent2.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far the environment is well prepared, so we can directly extract word embeddings from our finetuned word2vec, and throw then into InferSent to get phrase embeddings now.  \n",
    "There are four steps to achieve this.  \n",
    "1. Set up Word2Vec model\n",
    "2. Get all word embeddings\n",
    "3. Set up Inferset model\n",
    "4. Get phrase embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Word2Vec\n",
    "\n",
    "Now, in the first step, we have to extract all word embedding from Word2Vec model.  \n",
    "To get the w2v for our task, you can choose to [run training](#Training-by-yourself) (which needs about 10~20 mins), or simply skip it and [download our finetuned model](#Load-our-finetuned-model) in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training by yourself\n",
    "\n",
    "Since all utilities functions have been created above, we easily call them to set up our training pipeline.  \n",
    "1. load our training data\n",
    "2. create gensim's Word2Vec model\n",
    "3. load Google's pretrained model (This needs a while. 8~10 mins)\n",
    "4. finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "model loaded. time: 433.63464617729187 sec.\n",
      "training model...\n",
      "training finished\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "methodB_training_data = load_data('data/all_unhyphened_sent.txt')\n",
    "\n",
    "print('Creating model...')\n",
    "t = time()\n",
    "w2v_model, example_count = create_model(methodB_training_data)\n",
    "w2v_model = load_pretrained_model(w2v_model, 'models/GoogleNews-vectors-negative300.bin')\n",
    "print(f'model loaded. time: {time()-t} sec.')\n",
    "\n",
    "# train model\n",
    "print('training model...')\n",
    "train_model(methodB_training_data, w2v_model, example_count, epochs=5)\n",
    "print(f'training finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the model is trained, we will use only the word vector inside to extract vocabulary information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = w2v_model.wv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load our finetuned model\n",
    "If you have done the training by yourself, you can skip this section and go to [the next part](#Get-list-of-word-embeddings).  \n",
    "If the training wasn't done, please follow the codes below to download vocabulary weights trained on our task in [this Google Drive](https://drive.google.com/file/d/1iRj7OVlETT2mDXafm7JXCmvWpPhj7mAS/view?usp=sharing), and put all extracted contents unders `models/` folder.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1iRj7OVlETT2mDXafm7JXCmvWpPhj7mAS\n",
      "To: /home/nlplab/joaw/projects/TAAI_gen_paraphrase/models/unhyphened_model.tar.gz\n",
      "1.77GB [00:36, 48.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "# file size: 1.7G\n",
    "\n",
    "!gdown -O models/unhyphened_model.tar.gz --id 1iRj7OVlETT2mDXafm7JXCmvWpPhj7mAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unhyphened_model.kv\n",
      "unhyphened_model.kv.vectors.npy\n"
     ]
    }
   ],
   "source": [
    "!tar xzvf models/unhyphened_model.tar.gz -C models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load and use those vocubulary weight!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = KeyedVectors.load('models/unhyphened_model.kv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get list of word embeddings\n",
    "\n",
    "After the w2v model is prepared, we extrace word embeddings from our model now\\!   \n",
    "To make whole progress easier, we here create a function to extract all embeddings from a phrase string in advance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embeddings(model, phrase):\n",
    "    words = phrase.split(' ')\n",
    "    word_embeddings, unfound_words = [], []\n",
    "    for word in words:\n",
    "        try:\n",
    "            emb = model[word]\n",
    "            word_embeddings.append(emb)\n",
    "        except:\n",
    "            unfound_words.append(word)\n",
    "    return word_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple test to check results from `get_word_embeddings`.  \n",
    "You should see a list of embeddings as return.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "methodB_phrase1 = 'look for the'\n",
    "word_embs = get_word_embeddings(w2v, methodB_phrase1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(300,)\n"
     ]
    }
   ],
   "source": [
    "print(len(word_embs))\n",
    "print(word_embs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup InferSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create an InferSent object with specified parameters, and then we load pretrained model we downloaded above.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from infersent import InferSent\n",
    "\n",
    "# defaul config of infersent\n",
    "config = {'bsize': 64, \n",
    "          'word_emb_dim': 300, \n",
    "          'enc_lstm_dim': 2048,\n",
    "          'pool_type': 'max', \n",
    "          'dpout_model': 0.0, \n",
    "          'version': 2}\n",
    "\n",
    "infersent = InferSent(config)\n",
    "infersent.load_state_dict(torch.load('encoders/infersent2.pkl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get phrase embeddings\n",
    "\n",
    "Before we use InferSent model, we have to convert word embeddings into InferSent-compatible batch first. We create a function here to do the job.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_batch(word_embs):\n",
    "    # load beginning-of-sent and end-of-sent embedding\n",
    "    emb_bos = np.load(os.path.join('word_embs', 'bos.npy'))\n",
    "    emb_eos = np.load(os.path.join('word_embs', 'eos.npy'))\n",
    "    \n",
    "    # extract embeddings\n",
    "    lengths = len(word_embs) + 2\n",
    "    embeddings = np.vstack((emb_bos, np.array(word_embs), emb_eos))\n",
    "    \n",
    "    batch = np.zeros((lengths, 1, 300))\n",
    "    for i in range(len(embeddings)):\n",
    "        batch[i][0][:] = embeddings[i]\n",
    "    \n",
    "    return torch.FloatTensor(batch), np.array([lengths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the word embeddings extracted above to check the output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, length = transform_batch(word_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the tensor is prepared, we can extract phrase embeddings from InferSent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00746889 -0.06208688  0.0579672  ... -0.01622153 -0.02536337\n",
      "  -0.01013366]]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    methodB_pharse_emb1 = infersent.forward((batch, length)).data.cpu().numpy()\n",
    "print(methodB_pharse_emb1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method B - All in one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00746889 -0.08161387  0.05412931 ... -0.02657945 -0.02917238\n",
      "  -0.01013366]]\n"
     ]
    }
   ],
   "source": [
    "methodB_phrase2 = 'check out the'\n",
    "word_embs = get_word_embeddings(w2v, methodB_phrase2)\n",
    "batch, length = transform_batch(word_embs)\n",
    "with torch.no_grad():\n",
    "    methodB_pharse_emb2 = infersent.forward((batch, length)).data.cpu().numpy()\n",
    "print(methodB_pharse_emb2)\n",
    "\n",
    "# save embeddings in numpy-format if you want\n",
    "# out_path = 'phrase'\n",
    "# np.save(out_path, phrase_emb.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have two embeddings, we can compare their similarities with cosine similarities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    ret = np.inner(a, b) / (norm(a) * norm(b))\n",
    "    return 0.0 if np.isnan(ret) else float(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phrase1, phrase2 = methodA_pharse_emb1, methodA_pharse_emb2\n",
    "phrase1, phrase2 = methodB_pharse_emb1, methodB_pharse_emb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.894\n"
     ]
    }
   ],
   "source": [
    "print('{:.3f}'.format(cosine_similarity(phrase1, phrase2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the most similar phrases\n",
    "If you want to find the most similar phrases, we have to extract all phrases' embeddings and store them into a folder first. To keep the tutorial simple, we don't do this here in this notebook, you can refer to python scripts in [our GitHub](https://github.com/NTHU-NLPLAB/TAAI_gen_paraphrase) to do the job for you.  \n",
    "Once all phrases are stored in a folder, we can load them and do the comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load all phrase embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_folder = ''\n",
    "for filename in os.listdir(folder):\n",
    "    if filename in ['.', '..']: continue\n",
    "    bundle = os.path.splitext(filename)[0].replace('_', ' ')\n",
    "    emb = np.load(os.path.join(folder, filename), allow_pickle=True)\n",
    "    embeddings[bundle] = emb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a utility function to help us find and print the most similar phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(target, bundles, n=5):\n",
    "    similarities = []\n",
    "    target_emb = bundles[target]\n",
    "    for bundle, bundle_emb in bundles.items():\n",
    "        if bundle == target: continue\n",
    "        similarities.append((target, bundle, cosine_similarity(target_emb, bundle_emb)))\n",
    "    similarities.sort(key=lambda emb:-emb[2])\n",
    "    return similarities[:n]\n",
    "\n",
    "def print_similarity(tuples):\n",
    "    head = True\n",
    "    for t in tuples:\n",
    "        if head:\n",
    "            print(f'{t[0]}')\n",
    "            head = False\n",
    "        print(f'  > {t[1]}\\t{t[2]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can set up a interactive searching progress. Enjoy\\!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    query = input('input: ')\n",
    "    if query in ['quit', 'q']: breakn\n",
    "    print_similarity(most_similar(query, bundles_emb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 [tutorial]",
   "language": "python",
   "name": "taai_tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
